[{"authors":["admin"],"categories":null,"content":"I am an Assistant Professor in the Department of CSE, IIT Madras. Previously, I was a postdoc in Purdue University working with Prof. Suresh Jagannathan. Before that, I did my PhD from IISc under the guidance of Prof. YN Srikant.\nI am interested in developing Verification and Analysis techniques to improve the reliability, security and efficiency of Computer Systems. In this context, I am especially interested in solving theoretical and practical verification problems arising in Concurrent and Distributed Systems, Computer Architecture, Operating Systems and Real-time Systems.\nRecently, I have worked on developing automated verification techniques for programs running under a weakly consistent memory model in the context of Distributed Replicated systems and Distributed Databases. Previously, I have also worked on developing static timing analysis techniques, focusing on the impact of the Hardware cache hierarchy on the Worst Case Execution Time of programs.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an Assistant Professor in the Department of CSE, IIT Madras. Previously, I was a postdoc in Purdue University working with Prof. Suresh Jagannathan. Before that, I did my PhD from IISc under the guidance of Prof. YN Srikant.\nI am interested in developing Verification and Analysis techniques to improve the reliability, security and efficiency of Computer Systems. In this context, I am especially interested in solving theoretical and practical verification problems arising in Concurrent and Distributed Systems, Computer Architecture, Operating Systems and Real-time Systems.","tags":null,"title":"Kartik Nagar","type":"authors"},{"authors":null,"categories":null,"content":"Course Overview Automated Verification of programs has often been called the holy grail of computer science. While undecidable in general, over the years, a number of elegant techniques have been developed to solve this problem for several useful classes of programs. In today’s world, where the boundary between safety-critical and non safety-critical applications has become vanishingly thin and the volume and complexity of software has become staggeringly large, Automated Verification has started to gain more importance. In this course, we will go through a wide spectrum of techniques used in the domain of Automated Verification. The course will emphasise on both theory and practice, with hands-on experience of some of the tools used in this domain. In addition, the course will also involve reading the latest research papers in the area.\nCourse Contents   Preliminaries:SAT and SMT - Propositional Logic: Syntax and Semantics, Resolution-based SAT solving, DPLL, First-Order Logic: Syntax and Semantics, Satisfiability Modulo Theories, First Order Theories\n  Program Semantics and Specifications - Operational Semantics, Strongest Post-condition and Weakest Pre-condition, Hoare Logic\n  Automated Techniques - Abstract Interpretation, Bounded Model Checking, Predicate Abstraction, CEGAR, Property-Directed Reachability\n  Grading Policy (tentative)  Assignments: 30% Project: 40% Final Exam: 30%  Course Schedule  Assignments  Assignment 1: Out on Sep 28, Due Oct 7 Due Oct 11 Assignment 2: Out on Oct 26, Due Nov 4   Course Project  Proposal Due on Oct 18 Project Presentation in First Week of December Project Report Due on Dec 6   Final Exam  Take-home exam in the week of Dec 7    ","date":1595203200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1595203200,"objectID":"60ba97b1a65e1f2f5bb6b6560dba8296","permalink":"/courses/apv/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/courses/apv/","section":"courses","summary":"Course Overview Automated Verification of programs has often been called the holy grail of computer science. While undecidable in general, over the years, a number of elegant techniques have been developed to solve this problem for several useful classes of programs. In today’s world, where the boundary between safety-critical and non safety-critical applications has become vanishingly thin and the volume and complexity of software has become staggeringly large, Automated Verification has started to gain more importance.","tags":null,"title":"CS5030 - Automated Program Verification","type":"docs"},{"authors":null,"categories":null,"content":"   Topic Date(s) References/Additional Reading     Introduction 7/9 Hoare\u0026rsquo;s Paper. Intro Slides of Rajeev Alur\u0026rsquo;s Course on Computer Aided Verification   Propositional Logic: Introduction 8/9 BM Section 1.1-1.4   Propositional Logic: DPLL 9/9, 11/9 BM Section 1.6-1.7. Slides on CDCL-based SAT Solvers from Isil Dillig\u0026rsquo;s Course   First Order Logic: Introduction 11/9 BM Section 2.1-2.2   First Order Logic: Validity 14/9 BM Section 2.3,2.5   Satisfiability Modulo Theories: Introduction 15/9 BM Section 3.1,3.3   Satisfiability Modulo Theories: Theories 16/9,18/9 BM Section 3.2,3.3,3.6,3.8   First Order Logic: Compactness 21/9 BM Section 2.7.4   Satisfiability Modulo Theories: Bounded Model Checking 22/9    Z3 + Deductive Verification: Introduction 23/9    Operational Semantics 28/9 Ashutosh Gupta\u0026rsquo;s Lecture   Operational Semantics in FOL 29/9    Strongest Post-Condition 30/9,6/10,7/10 BM Section 12.1   Weakest Pre-Condition 9/10,12/10,13/10,14/10,16/10    Hoare Logic 16/10,19/10    Hoare Logic: VC Generation 20/10,21/10,27/10,28/10 BM Chapters 5,6   Dafny 28/10,2/11    Abstract Interpretation: Introduction and Lattice Theory 2/11,3/11,4/11,9/11,10/11 BM Section 12.1   Abstract Interpretation: Soundness 10/11,11/11,16/11    Abstract Interpretation: Kildall\u0026rsquo;s Algorithm 16/11,17/11     Video Lectures Link to Google Drive Folder containing all videos: APV Lectures\n","date":1599436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599436800,"objectID":"f4db3692de8c0185f6b1a8446825cf43","permalink":"/courses/apv/lectures/","publishdate":"2020-09-07T00:00:00Z","relpermalink":"/courses/apv/lectures/","section":"courses","summary":"Topic Date(s) References/Additional Reading     Introduction 7/9 Hoare\u0026rsquo;s Paper. Intro Slides of Rajeev Alur\u0026rsquo;s Course on Computer Aided Verification   Propositional Logic: Introduction 8/9 BM Section 1.1-1.4   Propositional Logic: DPLL 9/9, 11/9 BM Section 1.6-1.7. Slides on CDCL-based SAT Solvers from Isil Dillig\u0026rsquo;s Course   First Order Logic: Introduction 11/9 BM Section 2.1-2.2   First Order Logic: Validity 14/9 BM Section 2.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":"I am an avid reader (mostly fiction). I diligently keep track of all of my books at my Goodreads Account.\n","date":1595894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595894400,"objectID":"dda055c1b96739b21a95ca89d07548d9","permalink":"/personal/","publishdate":"2020-07-28T00:00:00Z","relpermalink":"/personal/","section":"","summary":"I am an avid reader (mostly fiction). I diligently keep track of all of my books at my Goodreads Account.","tags":null,"title":"Personal","type":"page"},{"authors":[],"categories":[],"content":"For global-scale applications such as Amazon, Twitter, Facebook, etc. with users distributed across the world, in order to provide a uniform low-latency and always available service, the application data needs to replicated at multiple servers across the world. Such replicated systems also facilitate other useful properties such as better scalability and fault tolerance, but they are very hard to program since they offer a completely different memory model to the programmers. Since the same data at different replicas can be concurrently updated and the updates can be applied in different orders at different replicas, this can result in subtle concurrency bugs for the applications running on top of replicated systems which can be hard to find through testing-based approaches. In this project, we proposed a number of automated verification techniques to reason about correctness of programs running on top of replicated systems.\nRepresentative Publications:  Semantics, Specification and Automated Verification of Concurrent Libraries in Replicated Systems. Kartik Nagar, Prasita Mukherjee and Suresh Jagannathan. CAV 20 CLOTHO : Directed Test Generation for Weakly Consistent Database Systems. Kia Rahmani, Kartik Nagar, Benjamin Delaware and Suresh Jagannathan. OOPSLA 19. Automated Parameterized Verification of CRDTs. Kartik Nagar and Suresh Jagannathan. CAV 19. Automated Detection of Serializability Violations under Weak Consistency. Kartik Nagar and Suresh Jagannathan. CONCUR 18. Alone Together: Compositional Reasoning and Inference for Weak Isolation. Gowtham Kaki, Kartik Nagar, Mahsa Najafzadeh and Suresh Jagannathan. POPL 18.  ","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"55e3b6edf839458e10643d9425356d98","permalink":"/project/replicated-systems/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/project/replicated-systems/","section":"project","summary":"For global-scale applications such as Amazon, Twitter, Facebook, etc. with users distributed across the world, in order to provide a uniform low-latency and always available service, the application data needs to replicated at multiple servers across the world. Such replicated systems also facilitate other useful properties such as better scalability and fault tolerance, but they are very hard to program since they offer a completely different memory model to the programmers.","tags":[],"title":"Automated Reasoning for Replicated Systems","type":"project"},{"authors":["Kartik Nagar","Prasita Mukherjee","Suresh Jagannathan"],"categories":[],"content":"","date":1586217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586217600,"objectID":"bcab7db049ae760d729f7b96d13edeb8","permalink":"/publication/cav20/","publishdate":"2020-04-07T15:26:22+05:30","relpermalink":"/publication/cav20/","section":"publication","summary":"Geo-replicated systems provide a number of desirable properties such as globally low latency, high availability, scalability, and built-in fault tolerance. Unfortunately, programming correct applications on top of such systems has proven to be very challenging, in large part because of the weak consistency guarantees they offer. These complexities are exacerbated when we try to adapt existing highly-performant concurrent libraries developed for shared-memory environments to this setting. The use of these libraries, developed with performance and scalability in mind, is highly desirable. But, identifying a suitable notion of correctness to check their validity under a weakly consistent execution model has not been well-studied, in large part because it is problematic to naively transplant criteria such as linearizability that has a useful interpretation in a shared-memory context to a distributed one where the cost of imposing a (logical) global ordering on all actions is prohibitive. In this paper, we tackle these issues by proposing appropriate semantics and specifications for highly-concurrent libraries in a weakly-consistent, replicated setting. We use these specifications to develop a static analysis framework that can automatically detect correctness violations of library implementations parameterized with respect to the different consistency policies provided by the underlying system. We use our framework to analyze the behavior of a number of highly non-trivial library implementations of stacks, queues, and exchangers. Our results provide the first demonstration that automated correctness checking of concurrent libraries in a weakly geo-replicated setting is both feasible and practical.","tags":["Replicated-Systems"],"title":"Semantics, Specification and Bounded Verification of Concurrent Libraries in Replicated Systems","type":"publication"},{"authors":["Kia Rahmani","Kartik Nagar","Benjamin Delaware","Suresh Jagannathan"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"a433a318d23838d7433fd153e23feb9d","permalink":"/publication/oopsla19/","publishdate":"2019-10-07T10:40:22-04:00","relpermalink":"/publication/oopsla19/","section":"publication","summary":"Relational database applications are notoriously difficult to test and debug. Concurrent execution of database transactions may violate complex structural invariants that constraint how changes to the contents of one (shared) table affect the contents of another. Simplifying the underlying concurrency model is one way to ameliorate the difficulty of understanding how concurrent accesses and updates can affect database state with respect to these sophisticated properties. Enforcing serializable execution of all transactions achieves this simplification, but it comes at a significant price in performance, especially at scale, where database state is often replicated to improve latency and availability. To address these challenges, this paper presents a novel testing framework for detecting serializability violations in (SQL) database-backed Java applications executing on weakly-consistent storage systems. We manifest our approach in a tool named CLOTHO, that combines a static analyzer and a model checker to generate abstract executions, discover serializability violations in these executions, and translate them back into concrete test inputs suitable for deployment in a test environment. To the best of our knowledge, CLOTHO is the first automated test generation facility for identifying serializability anomalies of Java applications intended to operate in geo-replicated distributed environments. An experimental evaluation on a set of industry-standard benchmarks demonstrates the utility of our approach.","tags":[],"title":"CLOTHO : Directed Test Generation for Weakly Consistent Database Systems","type":"publication"},{"authors":["Kartik Nagar","Suresh Jagannathan"],"categories":[],"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"fb5ba9ad462852ba05ea03291439ae08","permalink":"/publication/cav19/","publishdate":"2019-10-04T18:47:18-04:00","relpermalink":"/publication/cav19/","section":"publication","summary":"Maintaining multiple replicas of data is crucial to achieving scalability, availability and low latency in distributed applications. Conflict-free Replicated Data Types (CRDTs) are important building blocks in this domain because they are designed to operate correctly under the myriad behaviors possible in a weakly-consistent distributed setting. Because of the possibility of concurrent updates to the same object at different replicas, and the absence of any ordering guarantees on these updates, convergence is an important correctness criterion for CRDTs. This property asserts that two replicas which receive the same set of updates (in any order) must nonetheless converge to the same state. One way to prove that operations on a CRDT converge is to show that they commute since commutative actions by definition behave the same regardless of the order in which they execute. In this paper, we present a framework for automatically verifying convergence of CRDTs under different weak-consistency policies. Surprisingly, depending upon the consistency policy supported by the underlying system, we show that not all operations of a CRDT need to commute to achieve convergence. We develop a proof rule parameterized by a consistency specification based on the concepts of commutativity modulo consistency policy and non-interference to commutativity. We describe the design and implementation of a verification engine equipped with this rule and show how it can be used to provide the first automated convergence proofs for a number of challenging CRDTs, including sets, lists, and graphs.","tags":["Replicated-Systems"],"title":"Automated Parametrized Verification of CRDTs","type":"publication"},{"authors":["Kartik Nagar","Suresh Jagannathan"],"categories":[],"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"9fafae8cd8a5b2d37f45f29aabebb687","permalink":"/publication/concur18/","publishdate":"2019-10-04T18:42:01-04:00","relpermalink":"/publication/concur18/","section":"publication","summary":"While a number of weak consistency mechanisms have been developed in recent years to improve performance and ensure availability in distributed, replicated systems, ensuring the correctness of transactional applications running on top of such systems remains a difficult and important problem. Serializability is a well-understood correctness criterion for transactional programs; understanding whether applications are serializable when executed in a weakly-consistent environment, however remains a challenging exercise. In this work, we combine a dependency graph-based characterization of serializability and leverage the framework of abstract executions to develop a fully-automated approach for statically finding bounded serializability violations under any weak consistency model. We reduce the problem of serializability to satisfiability of a formula in First-Order Logic (FOL), which allows us to harness the power of existing SMT solvers. We provide rules to automatically construct the FOL encoding from programs written in SQL (allowing loops and conditionals) and express consistency specifications as FOL formula. In addition to detecting bounded serializability violations, we also provide two orthogonal schemes to reason about unbounded executions by providing sufficient conditions (again, in the form of FOL formulae) whose satisfiability implies the absence of anomalies in any arbitrary execution. We have applied the proposed technique on TPC-C, a real-world database program with complex application logic, and were able to discover anomalies under Parallel Snapshot Isolation (PSI), and verify serializability for unbounded executions under Snapshot Isolation (SI), two consistency mechanisms substantially weaker than serializability.","tags":[],"title":"Automated Detection of Serializability Violations under Weak Consistency","type":"publication"},{"authors":["Gowtham Kaki","Kartik Nagar","Mahsa Najafzadeh","Suresh Jagannathan"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"620964e755b5f17f19bb4874b687f13e","permalink":"/publication/popl18/","publishdate":"2019-10-04T18:21:41-04:00","relpermalink":"/publication/popl18/","section":"publication","summary":"Serializability is a well-understood correctness criterion that simplifies reasoning about the behavior of concurrent transactions by ensuring they are isolated from each other while they execute. However, enforcing serializable isolation comes at a steep cost in performance because it necessarily restricts opportunities to exploit concurrency even when such opportunities would not violate application-specific invariants. As a result, database systems in practice support, and often encourage, developers to implement transactions using weaker alternatives. These alternatives break the strong isolation guarantees offered by serializable transactions to permit greater concurrency. Unfortunately, the semantics of weak isolation is poorly understood, and usually explained only informally in terms of low-level implementation artifacts. Consequently, verifying high-level correctness properties in such environments remains a challenging problem. To address this issue, we present a novel program logic that enables compositional reasoning about the behavior of concurrently executing weakly-isolated transactions. Recognizing that the proof burden necessary to use this logic may dissuade application developers, we also describe an inference procedure based on this foundation that ascertains the weakest isolation level that still guarantees the safety of high-level consistency assertions associated with such transactions. The key to effective inference is the observation that weakly-isolated transactions can be viewed as functional (monadic) computations over an abstract database state, allowing us to treat their operations as state transformers over the database. This interpretation enables automated verification using off-the-shelf SMT solvers. Our development is parametric over a transaction’s specific isolation semantics, allowing it to be applicable over a range of concurrency control mechanisms. Case studies and experiments on real-world applications (written in an embedded DSL in OCaml) demonstrate the utility of our approach, and provide strong evidence that automated verification of weakly-isolated transactions can be placed on the same formal footing as their strongly-isolated serializable counterparts.","tags":[],"title":"Alone together: Compositional Reasoning and Inference for Weak Isolation","type":"publication"},{"authors":["Kartik Nagar","Y N Srikant"],"categories":[],"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"73cf65f308d78fe96a88f0ce00f34e1b","permalink":"/publication/tecs17/","publishdate":"2019-10-04T18:17:53-04:00","relpermalink":"/publication/tecs17/","section":"publication","summary":"Worst-Case Execution Time (WCET) is an important metric for programs running on real-time systems, and finding precise estimates of a program’s WCET is crucial to avoid wastage of hardware resources and to improve the schedulability of task sets. Caches have a major impact on a program’s execution time, and accurate estimation of a program’s cache behavior can lead to significant reduction in its estimated WCET. The traditional approach to cache analysis generally targets the worst-case cache behavior of individual cache accesses and provides a safe hit-miss classification for every individual access. In this work, we show that these classifications are not sufficient to precisely capture cache behavior, since they apply to individual accesses, and often, more precise predictions can be made about groups of accesses. Further, memory accesses inside loops may show the worst-case behavior only for a subset of the iteration space. In order to predict such behavior in a scalable fashion, we use the fact that the cache behavior of an access mostly depends only on the memory accesses made in the immediate vicinity, and hence we analyze a small, fixed-size neighborhood of every access with complete precision and summarize the resulting information in the form of cache miss paths. A variety of analyses are then performed on the cache miss paths to make precise predictions about cache behavior. We also demonstrate precision issues in Abstract Interpretation-based Must and Persistence cache analysis that can be easily solved using cache miss paths. Experimental results over a wide range of benchmarks demonstrate precision improvement in WCET of multipath programs over previous approaches, and we also show how to integrate our approach with other microarchitectural analysis such as pipeline analysis.","tags":[],"title":"Refining Cache Behaviour Prediction using Cache Miss Paths","type":"publication"},{"authors":["Kartik Nagar","Y N Srikant"],"categories":[],"content":"","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"284dffaebd4f3a1787f4ed6bd5af4858","permalink":"/publication/tecs16/","publishdate":"2019-10-04T18:11:22-04:00","relpermalink":"/publication/tecs16/","section":"publication","summary":"Real-time systems require a safe and precise estimate of the worst-case execution time (WCET) of programs. In multicore architectures, the precision of a program’s WCET estimate highly depends on the precision of its predicted shared cache behavior. Prediction of shared cache behavior is difficult due to the uncertain timing of interfering shared cache accesses made by programs running on other cores. Given the assignment of programs to cores, the worst-case interference placement (WCIP) technique tries to find the worst-case timing of interfering accesses, which would cause the maximum number of cache misses on the worst case path of the program, to determine its WCET. Although WCIP generates highly precise WCET estimates, the current ILP-based approach is also known to have very high analysis time. In this work, we investigate the WCIP problem in detail and determine its source of hardness. We show that performing WCIP is an NP-hard problem by reducing the 0-1 knapsack problem. We use this observation to make simplifying assumptions, which make the WCIP problem tractable, and we propose an approximate greedy technique for WCIP, whose time complexity is linear in the size of the program. We perform extensive experiments to show that the assumptions do not affect the precision of WCIP but result in significant reduction of analysis time.","tags":[],"title":"Fast and Precise Worst Case Interference Placement for Shared Cache Analysis","type":"publication"},{"authors":[],"categories":[],"content":"In Real-time systems, e.g. automobiles, aircrafts, space shuttles, robots, etc. programs are executed in response to external stimuli and must finish their execution before fixed deadlines to ensure correct behavior. Hence, Worst Case Execution Time (WCET) becomes a very important correctness criterion for programs running on real-time systems. In this project, we proposed static analysis techniques to determine the WCET of programs, chiefly focusing on the impact of caches on timing of programs.\nRepresentative Publications   Refining Cache Behaviour Prediction using Cache Miss Paths. Kartik Nagar and YN Srikant. TECS 17\n  Fast and Precise Worst Case Interference Placement for Shared Cache Analysis. Kartik Nagar and YN Srikant. TECS 16\n  Path-sensitive Cache Analysis using Cache Miss Paths. Kartik Nagar and YN Srikant. VMCAI 15\n  Precise Shared Cache Analysis using Optimal Interference Placement. Kartik Nagar and YN Srikant. RTAS 14\n  Interdependent Cache Analyses for better Precision and Safety. Kartik Nagar and YN Srikant. MEMOCODE 12\n  ","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"9a88d71086640fad1f8a9e7ed70c78a7","permalink":"/project/timing-analysis/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/project/timing-analysis/","section":"project","summary":"In Real-time systems, e.g. automobiles, aircrafts, space shuttles, robots, etc. programs are executed in response to external stimuli and must finish their execution before fixed deadlines to ensure correct behavior. Hence, Worst Case Execution Time (WCET) becomes a very important correctness criterion for programs running on real-time systems. In this project, we proposed static analysis techniques to determine the WCET of programs, chiefly focusing on the impact of caches on timing of programs.","tags":[],"title":"Timing Analysis for Real-time Systems","type":"project"},{"authors":["Kartik Nagar","Y N Srikant"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"84fb697057392c0a321983d93afbdae5","permalink":"/publication/vmcai15/","publishdate":"2019-10-04T18:15:00-04:00","relpermalink":"/publication/vmcai15/","section":"publication","summary":"Cache analysis plays a very important role in obtaining precise Worst Case Execution Time (WCET) estimates of programs for real-time systems. While Abstract Interpretation based approaches are almost universally used for cache analysis, they fail to take advantage of its unique requirement: it is not necessary to find the guaranteed cache behavior that holds across all executions of a program. We only need the cache behavior along one particular program path, which is the path with the maximum execution time. In this work, we introduce the concept of cache miss paths, which allows us to use the worst-case path information to improve the precision of AI-based cache analysis. We use Abstract Interpretation to determine the cache miss paths, and then integrate them in the IPET formulation. An added advantage is that this further allows us to use infeasible path information for cache analysis. Experimentally, our approach gives more precise WCETs as compared to AI-based cache analysis, and we also provide techniques to trade-off analysis time with precision to provide scalability.","tags":[],"title":"Path-sensitive Cache Analysis using Cache Miss Paths","type":"publication"},{"authors":["Kartik Nagar","Y N Srikant"],"categories":[],"content":"","date":1396310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396310400,"objectID":"ecac2f55994687421b748a09b1d33a80","permalink":"/publication/rtas14/","publishdate":"2019-10-04T18:05:26-04:00","relpermalink":"/publication/rtas14/","section":"publication","summary":"Determining the Worst Case Execution Time (WCET) of programs running on a multi-core architecture is a challenging problem, that is hampering the use of multi-cores in real-time systems. The highly imprecise WCET estimates obtained using the current state-of-the-art analyses has prompted research in the direction of making the multi-core architecture itself more estimation-friendly, but there has been little effort to make the WCET analysis more precise. The main difficulty in analyzing programs running on multi-core architectures arises from the fact that interferences to shared resources (such as shared cache) from other cores can occur at any time. Hence, to perform safe micro-architectural analysis, current approaches assume that all interferences occur at all times, which results in significantly imprecise analysis WCET estimates. However, since we are interested in the WCET, we can instead assume that the interferences will come at the worst possible program points, causing maximum increase in the execution time. In our work, we formulate an ILP problem to determine these worst case interference points, from the perspective of a shared cache, and determine the WCET by assuming that the interferences come at those program points. Our approach provides an average precision improvement of 25.63% over earlier analysis for benchmarks which perform a reasonable number of accesses to the shared cache.","tags":[],"title":"Precise Shared Cache Analysis using Optimal Interference Placement","type":"publication"},{"authors":["Kartik Nagar","Y N Srikant"],"categories":[],"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"a5d5e2a677fd22595c24adc5d13b34db","permalink":"/publication/memocode-12/","publishdate":"2012-07-01T00:00:00Z","relpermalink":"/publication/memocode-12/","section":"publication","summary":"One of the challenges for accurately estimating Worst Case Execution Time(WCET) of executables is to accurately predict their cache behavior. Various techniques have been developed to predict the cache contents at different program points to estimate the execution time of memory-accessing instructions. One of the most widely used techniques is Abstract Interpretation based Must Analysis, which determines the cache blocks guaranteed to be present in the cache, and hence provides safe estimation of cache hits and misses. However, Must Analysis is highly imprecise, and platforms using Must Analysis have been known to produce blown-up WCET estimates. In our work, we propose to use May Analysis to assist the Must Analysis cache update and make it more precise. We prove the safety of our approach as well as provide examples where our Improved Must Analysis provides better precision. Further, we also detect a serious flaw in the original Persistence Analysis, and use Must and May Analysis to assist the Persistence Analysis cache update, to make it safe and more precise than the known solutions to the problem. Finally, we propose an improvement in the original May Analysis, to make it more precise, especially for Data Cache Analysis.","tags":[],"title":"Interdependent Cache Analyses for better precision and safety","type":"publication"}]